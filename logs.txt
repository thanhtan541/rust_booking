
==> Audit <==
|--------------|-----------------------|----------|------|---------|---------------------|---------------------|
|   Command    |         Args          | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|-----------------------|----------|------|---------|---------------------|---------------------|
| update-check |                       | minikube | loki | v1.30.1 | 05 Jul 23 15:29 +07 | 05 Jul 23 15:29 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 06 Jul 23 10:24 +07 | 06 Jul 23 10:24 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 06 Jul 23 16:38 +07 | 06 Jul 23 16:38 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 11 Jul 23 09:42 +07 | 11 Jul 23 09:42 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 08 Aug 23 10:32 +07 | 08 Aug 23 10:32 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 23 Aug 23 14:53 +07 | 23 Aug 23 14:53 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 24 Aug 23 18:46 +07 | 24 Aug 23 18:46 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 30 Aug 23 15:33 +07 | 30 Aug 23 15:33 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 30 Aug 23 17:07 +07 | 30 Aug 23 17:07 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 06 Sep 23 10:40 +07 | 06 Sep 23 10:40 +07 |
| update-check |                       | minikube | loki | v1.31.2 | 08 Sep 23 09:47 +07 | 08 Sep 23 09:47 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 04 Oct 23 19:51 +07 | 04 Oct 23 19:51 +07 |
| update-check |                       | minikube | loki | v1.30.1 | 12 Oct 23 14:34 +07 | 12 Oct 23 14:34 +07 |
| start        |                       | minikube | loki | v1.31.2 | 15 Oct 23 14:07 +07 | 15 Oct 23 14:14 +07 |
| docker-env   |                       | minikube | loki | v1.31.2 | 15 Oct 23 16:06 +07 | 15 Oct 23 16:06 +07 |
| docker-env   |                       | minikube | loki | v1.31.2 | 15 Oct 23 16:06 +07 | 15 Oct 23 16:06 +07 |
| service      | hello-minikube1 --url | minikube | loki | v1.31.2 | 15 Oct 23 16:47 +07 |                     |
| service      | hello-minikube1 --url | minikube | loki | v1.31.2 | 15 Oct 23 16:48 +07 |                     |
| service      | hello-minikube1       | minikube | loki | v1.31.2 | 15 Oct 23 16:52 +07 |                     |
| service      | hello-minikube1 --url | minikube | loki | v1.31.2 | 15 Oct 23 16:56 +07 |                     |
| dashboard    | --url                 | minikube | loki | v1.31.2 | 15 Oct 23 16:58 +07 |                     |
| service      | hello-minikube1 --url | minikube | loki | v1.31.2 | 15 Oct 23 17:04 +07 |                     |
| service      | web --url             | minikube | loki | v1.31.2 | 15 Oct 23 17:07 +07 |                     |
| service      | app-b --url           | minikube | loki | v1.31.2 | 15 Oct 23 17:07 +07 |                     |
| service      | web --url             | minikube | loki | v1.31.2 | 15 Oct 23 17:09 +07 |                     |
| service      | web --url             | minikube | loki | v1.31.2 | 15 Oct 23 17:10 +07 |                     |
|--------------|-----------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2023/10/15 14:07:08
Running on machine: PF3LTQTA
Binary: Built with gc go1.20.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1015 14:07:08.003805   14117 out.go:296] Setting OutFile to fd 1 ...
I1015 14:07:08.003915   14117 out.go:309] Setting ErrFile to fd 2...
I1015 14:07:08.004124   14117 root.go:338] Updating PATH: /home/loki/.minikube/bin
W1015 14:07:08.004926   14117 root.go:314] Error reading config file at /home/loki/.minikube/config/config.json: open /home/loki/.minikube/config/config.json: no such file or directory
I1015 14:07:08.005826   14117 out.go:303] Setting JSON to false
I1015 14:07:08.011808   14117 start.go:128] hostinfo: {"hostname":"PF3LTQTA","uptime":21449,"bootTime":1697332179,"procs":22,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.90.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"63e53c8a-cb3d-4805-29fc-6bc0641ee44f"}
I1015 14:07:08.011852   14117 start.go:138] virtualization:  guest
I1015 14:07:08.014576   14117 out.go:177] 😄  minikube v1.31.2 on Ubuntu 20.04 (amd64)
I1015 14:07:08.022795   14117 driver.go:373] Setting default libvirt URI to qemu:///system
I1015 14:07:08.022831   14117 global.go:111] Querying for installed drivers using PATH=/home/loki/.minikube/bin:/home/loki/bin:/home/loki/.cargo/bin:/home/loki/bin:/home/loki/.sdkman/candidates/java/current/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/Microsoft SQL Server/150/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/:/mnt/c/Program Files/Neovim/bin:/mnt/c/Users/nttan/AppData/Roaming/nvm:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Gradle/bin:/mnt/c/Program Files/dotnet/:/Docker/host/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Users/nttan/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/nttan/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/nttan/AppData/Local/Programs/oh-my-posh/bin:/mnt/c/Users/nttan/apache-maven-3.8.7/bin:/mnt/c/Users/nttan/.dotnet/tools:/mnt/c/Program Files/sfdx/bin:/mnt/c/Program Files/JetBrains/PyCharm 2022.3.2/bin:/snap/bin:/home/loki/.dotnet/tools:/home/loki/.fzf/bin:/home/loki/.dotnet/tools
I1015 14:07:08.022621   14117 notify.go:220] Checking for updates...
W1015 14:07:08.023011   14117 preload.go:295] Failed to list preload files: open /home/loki/.minikube/cache/preloaded-tarball: no such file or directory
I1015 14:07:08.145477   14117 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1015 14:07:08.510927   14117 docker.go:121] docker version: linux-24.0.5:Docker Desktop
I1015 14:07:08.511116   14117 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1015 14:07:10.263559   14117 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.752301164s)
I1015 14:07:10.264183   14117 info.go:266] docker info: {ID:7dacc2bf-dcc4-4f7b-8e93-4929a710fb0e Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:73 SystemTime:2023-10-15 07:07:10.223765801 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:33036718080 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I1015 14:07:10.264523   14117 docker.go:294] overlay module found
I1015 14:07:10.264533   14117 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1015 14:07:10.403092   14117 global.go:122] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1015 14:07:10.433101   14117 global.go:122] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1015 14:07:10.563979   14117 global.go:122] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1015 14:07:10.681854   14117 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1015 14:07:10.681888   14117 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1015 14:07:10.935399   14117 global.go:122] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1015 14:07:10.935441   14117 driver.go:308] not recommending "none" due to default: false
I1015 14:07:10.935445   14117 driver.go:308] not recommending "ssh" due to default: false
I1015 14:07:10.935458   14117 driver.go:343] Picked: docker
I1015 14:07:10.935482   14117 driver.go:344] Alternatives: [none ssh]
I1015 14:07:10.935486   14117 driver.go:345] Rejects: [vmware kvm2 podman qemu2 virtualbox]
I1015 14:07:10.943430   14117 out.go:177] ✨  Automatically selected the docker driver. Other choices: none, ssh
I1015 14:07:10.945239   14117 start.go:298] selected driver: docker
I1015 14:07:10.945246   14117 start.go:902] validating driver "docker" against <nil>
I1015 14:07:10.945267   14117 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1015 14:07:10.945380   14117 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1015 14:07:11.233471   14117 info.go:266] docker info: {ID:7dacc2bf-dcc4-4f7b-8e93-4929a710fb0e Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:73 SystemTime:2023-10-15 07:07:11.193591005 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:33036718080 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I1015 14:07:11.233713   14117 start_flags.go:305] no existing cluster config was found, will generate one from the flags 
I1015 14:07:11.235799   14117 start_flags.go:382] Using suggested 7800MB memory alloc based on sys=31506MB, container=31506MB
I1015 14:07:11.236161   14117 start_flags.go:901] Wait components to verify : map[apiserver:true system_pods:true]
I1015 14:07:11.246393   14117 out.go:177] 📌  Using Docker driver with root privileges
W1015 14:07:11.249019   14117 out.go:239] ❗  For an improved experience it's recommended to use Docker Engine instead of Docker Desktop.
Docker Engine installation instructions: https://docs.docker.com/engine/install/#server
I1015 14:07:11.249083   14117 cni.go:84] Creating CNI manager for ""
I1015 14:07:11.249099   14117 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1015 14:07:11.249111   14117 start_flags.go:314] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1015 14:07:11.249120   14117 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:7800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/loki:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1015 14:07:11.257540   14117 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1015 14:07:11.263491   14117 cache.go:122] Beginning downloading kic base image for docker with docker
I1015 14:07:11.265681   14117 out.go:177] 🚜  Pulling base image ...
I1015 14:07:11.267897   14117 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1015 14:07:11.267949   14117 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1015 14:07:11.444387   14117 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I1015 14:07:11.444795   14117 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I1015 14:07:11.453734   14117 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I1015 14:07:11.463850   14117 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.27.4/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1015 14:07:11.463896   14117 cache.go:57] Caching tarball of preloaded images
I1015 14:07:11.464165   14117 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1015 14:07:11.467922   14117 out.go:177] 💾  Downloading Kubernetes v1.27.4 preload ...
I1015 14:07:11.474124   14117 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 ...
I1015 14:07:11.934066   14117 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.27.4/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4?checksum=md5:57da30b73c6409bef80873fc9e1b0d5b -> /home/loki/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1015 14:08:38.962266   14117 preload.go:249] saving checksum for preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 ...
I1015 14:08:38.962371   14117 preload.go:256] verifying checksum of /home/loki/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 ...
I1015 14:08:40.391412   14117 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1015 14:08:40.391704   14117 profile.go:148] Saving config to /home/loki/.minikube/profiles/minikube/config.json ...
I1015 14:08:40.391722   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/config.json: {Name:mkaa923b4a5de962353a56c0126adbd7da8afe00 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:12:41.141502   14117 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
I1015 14:12:41.141514   14117 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from local cache
I1015 14:13:13.640642   14117 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from cached tarball
I1015 14:13:13.640723   14117 cache.go:195] Successfully downloaded all kic artifacts
I1015 14:13:13.640795   14117 start.go:365] acquiring machines lock for minikube: {Name:mk4b3c54f4f7ecaa39c7a6b7d880aacdcb623f05 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1015 14:13:13.641128   14117 start.go:369] acquired machines lock for "minikube" in 299.202µs
I1015 14:13:13.641909   14117 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:7800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/loki:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0} &{Name: IP: Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1015 14:13:13.642024   14117 start.go:125] createHost starting for "" (driver="docker")
I1015 14:13:13.649605   14117 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=7800MB) ...
I1015 14:13:13.650006   14117 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1015 14:13:13.650044   14117 client.go:168] LocalClient.Create starting
I1015 14:13:13.650675   14117 main.go:141] libmachine: Creating CA: /home/loki/.minikube/certs/ca.pem
I1015 14:13:14.098419   14117 main.go:141] libmachine: Creating client certificate: /home/loki/.minikube/certs/cert.pem
I1015 14:13:14.542101   14117 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1015 14:13:14.727960   14117 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1015 14:13:14.728037   14117 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I1015 14:13:14.728050   14117 cli_runner.go:164] Run: docker network inspect minikube
W1015 14:13:14.892635   14117 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1015 14:13:14.892659   14117 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1015 14:13:14.892671   14117 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1015 14:13:14.899403   14117 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1015 14:13:15.068264   14117 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0019772d0}
I1015 14:13:15.068331   14117 network_create.go:123] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1015 14:13:15.068503   14117 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1015 14:13:15.328934   14117 network_create.go:107] docker network minikube 192.168.49.0/24 created
I1015 14:13:15.328960   14117 kic.go:117] calculated static IP "192.168.49.2" for the "minikube" container
I1015 14:13:15.329042   14117 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1015 14:13:15.528421   14117 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1015 14:13:15.720302   14117 oci.go:103] Successfully created a docker volume minikube
I1015 14:13:15.720482   14117 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib
I1015 14:13:20.738514   14117 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib: (5.017991581s)
I1015 14:13:20.738534   14117 oci.go:107] Successfully prepared a docker volume minikube
I1015 14:13:20.738579   14117 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1015 14:13:20.738601   14117 kic.go:190] Starting extracting preloaded images to volume ...
I1015 14:13:20.738670   14117 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/loki/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir
I1015 14:13:33.341521   14117 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/loki/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir: (12.602795934s)
I1015 14:13:33.341659   14117 kic.go:199] duration metric: took 12.603045 seconds to extract preloaded images to volume
W1015 14:13:33.342268   14117 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1015 14:13:33.342471   14117 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1015 14:13:33.758439   14117 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7800mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631
I1015 14:13:36.328743   14117 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7800mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631: (2.570238428s)
I1015 14:13:36.328888   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1015 14:13:36.640658   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 14:13:36.887606   14117 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1015 14:13:37.400365   14117 oci.go:144] the created container "minikube" has a running status.
I1015 14:13:37.400402   14117 kic.go:221] Creating ssh key for kic: /home/loki/.minikube/machines/minikube/id_rsa...
I1015 14:13:38.420539   14117 kic_runner.go:191] docker (temp): /home/loki/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1015 14:13:38.760889   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 14:13:39.001506   14117 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1015 14:13:39.001529   14117 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1015 14:13:39.491199   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 14:13:39.789132   14117 machine.go:88] provisioning docker machine ...
I1015 14:13:39.789175   14117 ubuntu.go:169] provisioning hostname "minikube"
I1015 14:13:39.789294   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:40.060608   14117 main.go:141] libmachine: Using SSH client type: native
I1015 14:13:40.061085   14117 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 57059 <nil> <nil>}
I1015 14:13:40.061094   14117 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1015 14:13:40.330161   14117 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1015 14:13:40.330360   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:40.658717   14117 main.go:141] libmachine: Using SSH client type: native
I1015 14:13:40.659666   14117 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 57059 <nil> <nil>}
I1015 14:13:40.659701   14117 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1015 14:13:40.889524   14117 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1015 14:13:40.889547   14117 ubuntu.go:175] set auth options {CertDir:/home/loki/.minikube CaCertPath:/home/loki/.minikube/certs/ca.pem CaPrivateKeyPath:/home/loki/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/loki/.minikube/machines/server.pem ServerKeyPath:/home/loki/.minikube/machines/server-key.pem ClientKeyPath:/home/loki/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/loki/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/loki/.minikube}
I1015 14:13:40.889569   14117 ubuntu.go:177] setting up certificates
I1015 14:13:40.889579   14117 provision.go:83] configureAuth start
I1015 14:13:40.889690   14117 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1015 14:13:41.222504   14117 provision.go:138] copyHostCerts
I1015 14:13:41.222616   14117 exec_runner.go:151] cp: /home/loki/.minikube/certs/ca.pem --> /home/loki/.minikube/ca.pem (1074 bytes)
I1015 14:13:41.222808   14117 exec_runner.go:151] cp: /home/loki/.minikube/certs/cert.pem --> /home/loki/.minikube/cert.pem (1115 bytes)
I1015 14:13:41.222920   14117 exec_runner.go:151] cp: /home/loki/.minikube/certs/key.pem --> /home/loki/.minikube/key.pem (1675 bytes)
I1015 14:13:41.223002   14117 provision.go:112] generating server cert: /home/loki/.minikube/machines/server.pem ca-key=/home/loki/.minikube/certs/ca.pem private-key=/home/loki/.minikube/certs/ca-key.pem org=loki.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1015 14:13:41.592869   14117 provision.go:172] copyRemoteCerts
I1015 14:13:41.592946   14117 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1015 14:13:41.592998   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:41.882409   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:13:42.091540   14117 ssh_runner.go:362] scp /home/loki/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1015 14:13:42.222633   14117 ssh_runner.go:362] scp /home/loki/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1015 14:13:42.353371   14117 ssh_runner.go:362] scp /home/loki/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1015 14:13:42.495977   14117 provision.go:86] duration metric: configureAuth took 1.602771157s
I1015 14:13:42.496003   14117 ubuntu.go:193] setting minikube options for container-runtime
I1015 14:13:42.496268   14117 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1015 14:13:42.501073   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:42.801975   14117 main.go:141] libmachine: Using SSH client type: native
I1015 14:13:42.802898   14117 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 57059 <nil> <nil>}
I1015 14:13:42.802915   14117 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1015 14:13:43.043693   14117 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1015 14:13:43.043711   14117 ubuntu.go:71] root file system type: overlay
I1015 14:13:43.043919   14117 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1015 14:13:43.044039   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:43.315068   14117 main.go:141] libmachine: Using SSH client type: native
I1015 14:13:43.315629   14117 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 57059 <nil> <nil>}
I1015 14:13:43.315768   14117 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1015 14:13:43.613987   14117 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1015 14:13:43.614099   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:43.884736   14117 main.go:141] libmachine: Using SSH client type: native
I1015 14:13:43.885345   14117 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 57059 <nil> <nil>}
I1015 14:13:43.885379   14117 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1015 14:13:45.452269   14117 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-07 14:50:55.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-10-15 07:13:43.581079872 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1015 14:13:45.452300   14117 machine.go:91] provisioned docker machine in 5.659540084s
I1015 14:13:45.452313   14117 client.go:171] LocalClient.Create took 31.798655852s
I1015 14:13:45.452362   14117 start.go:167] duration metric: libmachine.API.Create for "minikube" took 31.798745572s
I1015 14:13:45.452370   14117 start.go:300] post-start starting for "minikube" (driver="docker")
I1015 14:13:45.452383   14117 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1015 14:13:45.452464   14117 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1015 14:13:45.452515   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:45.711812   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:13:45.865975   14117 ssh_runner.go:195] Run: cat /etc/os-release
I1015 14:13:45.874437   14117 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1015 14:13:45.874458   14117 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1015 14:13:45.874470   14117 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1015 14:13:45.874479   14117 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1015 14:13:45.874493   14117 filesync.go:126] Scanning /home/loki/.minikube/addons for local assets ...
I1015 14:13:45.881010   14117 filesync.go:126] Scanning /home/loki/.minikube/files for local assets ...
I1015 14:13:45.883381   14117 start.go:303] post-start completed in 430.997195ms
I1015 14:13:45.883888   14117 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1015 14:13:46.122177   14117 profile.go:148] Saving config to /home/loki/.minikube/profiles/minikube/config.json ...
I1015 14:13:46.122503   14117 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1015 14:13:46.122585   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:46.342120   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:13:46.474297   14117 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1015 14:13:46.484024   14117 start.go:128] duration metric: createHost completed in 32.838367353s
I1015 14:13:46.484040   14117 start.go:83] releasing machines lock for "minikube", held for 32.839294188s
I1015 14:13:46.484146   14117 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1015 14:13:46.692159   14117 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1015 14:13:46.692159   14117 ssh_runner.go:195] Run: cat /version.json
I1015 14:13:46.692217   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:46.692222   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:13:46.924894   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:13:46.924907   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:13:47.055737   14117 ssh_runner.go:195] Run: systemctl --version
I1015 14:13:47.465689   14117 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1015 14:13:47.475627   14117 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1015 14:13:47.581368   14117 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1015 14:13:47.581488   14117 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1015 14:13:47.692255   14117 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1015 14:13:47.692287   14117 start.go:466] detecting cgroup driver to use...
I1015 14:13:47.692324   14117 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1015 14:13:47.692427   14117 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1015 14:13:47.753580   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1015 14:13:47.802563   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1015 14:13:47.855355   14117 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1015 14:13:47.855466   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1015 14:13:47.902134   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1015 14:13:47.952492   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1015 14:13:47.985668   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1015 14:13:48.041074   14117 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1015 14:13:48.074830   14117 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1015 14:13:48.114800   14117 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1015 14:13:48.165669   14117 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1015 14:13:48.206165   14117 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 14:13:48.436048   14117 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1015 14:13:48.635336   14117 start.go:466] detecting cgroup driver to use...
I1015 14:13:48.635387   14117 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1015 14:13:48.635534   14117 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1015 14:13:48.691708   14117 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1015 14:13:48.691800   14117 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1015 14:13:48.762609   14117 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1015 14:13:48.871359   14117 ssh_runner.go:195] Run: which cri-dockerd
I1015 14:13:48.892022   14117 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1015 14:13:48.954453   14117 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1015 14:13:49.014741   14117 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1015 14:13:49.343343   14117 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1015 14:13:49.682738   14117 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I1015 14:13:49.682760   14117 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I1015 14:13:49.794672   14117 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 14:13:49.965509   14117 ssh_runner.go:195] Run: sudo systemctl restart docker
I1015 14:13:50.601614   14117 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1015 14:13:50.824457   14117 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1015 14:13:51.024635   14117 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1015 14:13:51.243443   14117 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 14:13:51.413319   14117 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1015 14:13:51.482001   14117 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1015 14:13:51.675273   14117 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1015 14:13:51.873243   14117 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1015 14:13:51.873380   14117 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1015 14:13:51.882923   14117 start.go:534] Will wait 60s for crictl version
I1015 14:13:51.882985   14117 ssh_runner.go:195] Run: which crictl
I1015 14:13:51.892335   14117 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1015 14:13:52.042499   14117 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1015 14:13:52.042591   14117 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1015 14:13:52.121159   14117 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1015 14:13:52.203792   14117 out.go:204] 🐳  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1015 14:13:52.204027   14117 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1015 14:13:52.411456   14117 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1015 14:13:52.415936   14117 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1015 14:13:52.454675   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1015 14:13:52.665588   14117 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1015 14:13:52.665662   14117 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1015 14:13:52.715364   14117 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1015 14:13:52.715380   14117 docker.go:566] Images already preloaded, skipping extraction
I1015 14:13:52.715496   14117 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1015 14:13:52.756208   14117 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1015 14:13:52.756224   14117 cache_images.go:84] Images are preloaded, skipping loading
I1015 14:13:52.760976   14117 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1015 14:13:52.891655   14117 cni.go:84] Creating CNI manager for ""
I1015 14:13:52.891680   14117 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1015 14:13:52.891697   14117 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1015 14:13:52.891727   14117 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1015 14:13:52.891970   14117 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1015 14:13:52.892108   14117 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1015 14:13:52.892214   14117 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1015 14:13:52.915776   14117 binaries.go:44] Found k8s binaries, skipping transfer
I1015 14:13:52.915849   14117 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1015 14:13:52.951464   14117 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1015 14:13:53.021016   14117 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1015 14:13:53.100981   14117 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1015 14:13:53.162471   14117 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1015 14:13:53.165719   14117 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1015 14:13:53.212872   14117 certs.go:56] Setting up /home/loki/.minikube/profiles/minikube for IP: 192.168.49.2
I1015 14:13:53.212904   14117 certs.go:190] acquiring lock for shared ca certs: {Name:mk5478d86bff9d6fb7389626afad4cd5c1b41c18 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:53.213088   14117 certs.go:204] generating minikubeCA CA: /home/loki/.minikube/ca.key
I1015 14:13:53.525873   14117 crypto.go:156] Writing cert to /home/loki/.minikube/ca.crt ...
I1015 14:13:53.525888   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/ca.crt: {Name:mk6886e24d3cb3ff6db7702eecdc33b102aa205a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:53.526197   14117 crypto.go:164] Writing key to /home/loki/.minikube/ca.key ...
I1015 14:13:53.526210   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/ca.key: {Name:mkadd96bfea7a788fb7e009df24924bc52dedb8c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:53.531132   14117 certs.go:204] generating proxyClientCA CA: /home/loki/.minikube/proxy-client-ca.key
I1015 14:13:53.743851   14117 crypto.go:156] Writing cert to /home/loki/.minikube/proxy-client-ca.crt ...
I1015 14:13:53.743870   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/proxy-client-ca.crt: {Name:mk1a9121b55be8c4537656ef0b21b5ffa5bf2ee8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:53.744164   14117 crypto.go:164] Writing key to /home/loki/.minikube/proxy-client-ca.key ...
I1015 14:13:53.744171   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/proxy-client-ca.key: {Name:mkee5d4aa9ace731df3ac412ee9f0d210774268b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:53.744392   14117 certs.go:319] generating minikube-user signed cert: /home/loki/.minikube/profiles/minikube/client.key
I1015 14:13:53.744402   14117 crypto.go:68] Generating cert /home/loki/.minikube/profiles/minikube/client.crt with IP's: []
I1015 14:13:54.555008   14117 crypto.go:156] Writing cert to /home/loki/.minikube/profiles/minikube/client.crt ...
I1015 14:13:54.555026   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/client.crt: {Name:mk9a60445109705772d2d2329dd1f42abac59f31 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:54.555242   14117 crypto.go:164] Writing key to /home/loki/.minikube/profiles/minikube/client.key ...
I1015 14:13:54.555247   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/client.key: {Name:mkc4333e1b34dd435111b0365280455e645ed1b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:54.555353   14117 certs.go:319] generating minikube signed cert: /home/loki/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1015 14:13:54.555364   14117 crypto.go:68] Generating cert /home/loki/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1015 14:13:54.791641   14117 crypto.go:156] Writing cert to /home/loki/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1015 14:13:54.791659   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk5486b1ddff530a105d4fbeb66a43cca8d65e4a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:54.791899   14117 crypto.go:164] Writing key to /home/loki/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1015 14:13:54.791906   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk690cda654171beb92c4cc130f84464e8665bb5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:54.792009   14117 certs.go:337] copying /home/loki/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/loki/.minikube/profiles/minikube/apiserver.crt
I1015 14:13:54.792066   14117 certs.go:341] copying /home/loki/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/loki/.minikube/profiles/minikube/apiserver.key
I1015 14:13:54.792153   14117 certs.go:319] generating aggregator signed cert: /home/loki/.minikube/profiles/minikube/proxy-client.key
I1015 14:13:54.792166   14117 crypto.go:68] Generating cert /home/loki/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1015 14:13:54.962572   14117 crypto.go:156] Writing cert to /home/loki/.minikube/profiles/minikube/proxy-client.crt ...
I1015 14:13:54.962588   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/proxy-client.crt: {Name:mk214a012c4625fe81fc0d27e944311e45fc96eb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:54.962776   14117 crypto.go:164] Writing key to /home/loki/.minikube/profiles/minikube/proxy-client.key ...
I1015 14:13:54.962781   14117 lock.go:35] WriteFile acquiring /home/loki/.minikube/profiles/minikube/proxy-client.key: {Name:mk46ddb27a84542b75547e5a7ea9e56607fc11fd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:13:54.962948   14117 certs.go:437] found cert: /home/loki/.minikube/certs/home/loki/.minikube/certs/ca-key.pem (1679 bytes)
I1015 14:13:54.962972   14117 certs.go:437] found cert: /home/loki/.minikube/certs/home/loki/.minikube/certs/ca.pem (1074 bytes)
I1015 14:13:54.962989   14117 certs.go:437] found cert: /home/loki/.minikube/certs/home/loki/.minikube/certs/cert.pem (1115 bytes)
I1015 14:13:54.963004   14117 certs.go:437] found cert: /home/loki/.minikube/certs/home/loki/.minikube/certs/key.pem (1675 bytes)
I1015 14:13:54.963472   14117 ssh_runner.go:362] scp /home/loki/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1015 14:13:55.043021   14117 ssh_runner.go:362] scp /home/loki/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1015 14:13:55.134479   14117 ssh_runner.go:362] scp /home/loki/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1015 14:13:55.221107   14117 ssh_runner.go:362] scp /home/loki/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1015 14:13:55.315685   14117 ssh_runner.go:362] scp /home/loki/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1015 14:13:55.413325   14117 ssh_runner.go:362] scp /home/loki/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1015 14:13:55.522421   14117 ssh_runner.go:362] scp /home/loki/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1015 14:13:55.604752   14117 ssh_runner.go:362] scp /home/loki/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1015 14:13:55.674490   14117 ssh_runner.go:362] scp /home/loki/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1015 14:13:55.782300   14117 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1015 14:13:55.855064   14117 ssh_runner.go:195] Run: openssl version
I1015 14:13:55.865742   14117 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1015 14:13:55.905552   14117 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1015 14:13:55.923613   14117 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Oct 15 07:13 /usr/share/ca-certificates/minikubeCA.pem
I1015 14:13:55.923687   14117 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1015 14:13:55.934016   14117 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1015 14:13:55.964354   14117 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1015 14:13:55.972645   14117 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I1015 14:13:55.972678   14117 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:7800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/loki:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1015 14:13:55.972799   14117 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1015 14:13:56.013689   14117 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1015 14:13:56.046210   14117 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1015 14:13:56.094084   14117 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I1015 14:13:56.094185   14117 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1015 14:13:56.122812   14117 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1015 14:13:56.122849   14117 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1015 14:13:56.373287   14117 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I1015 14:13:56.544781   14117 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1015 14:14:26.471933   14117 kubeadm.go:322] [init] Using Kubernetes version: v1.27.4
I1015 14:14:26.472069   14117 kubeadm.go:322] [preflight] Running pre-flight checks
I1015 14:14:26.472198   14117 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I1015 14:14:26.472355   14117 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1015 14:14:26.472511   14117 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1015 14:14:26.472614   14117 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1015 14:14:26.480732   14117 out.go:204]     ▪ Generating certificates and keys ...
I1015 14:14:26.480889   14117 kubeadm.go:322] [certs] Using existing ca certificate authority
I1015 14:14:26.481040   14117 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I1015 14:14:26.481190   14117 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I1015 14:14:26.481277   14117 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I1015 14:14:26.481357   14117 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I1015 14:14:26.481455   14117 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I1015 14:14:26.481587   14117 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I1015 14:14:26.481774   14117 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1015 14:14:26.481893   14117 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I1015 14:14:26.482129   14117 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1015 14:14:26.482237   14117 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I1015 14:14:26.482317   14117 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I1015 14:14:26.482387   14117 kubeadm.go:322] [certs] Generating "sa" key and public key
I1015 14:14:26.482489   14117 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1015 14:14:26.482583   14117 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I1015 14:14:26.482706   14117 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1015 14:14:26.482825   14117 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1015 14:14:26.482910   14117 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1015 14:14:26.483085   14117 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1015 14:14:26.483232   14117 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1015 14:14:26.483296   14117 kubeadm.go:322] [kubelet-start] Starting the kubelet
I1015 14:14:26.483437   14117 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1015 14:14:26.492850   14117 out.go:204]     ▪ Booting up control plane ...
I1015 14:14:26.493041   14117 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1015 14:14:26.493153   14117 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1015 14:14:26.493248   14117 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1015 14:14:26.493371   14117 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1015 14:14:26.493619   14117 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1015 14:14:26.493739   14117 kubeadm.go:322] [apiclient] All control plane components are healthy after 22.502973 seconds
I1015 14:14:26.493913   14117 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1015 14:14:26.494107   14117 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1015 14:14:26.498877   14117 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I1015 14:14:26.499009   14117 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1015 14:14:26.499050   14117 kubeadm.go:322] [bootstrap-token] Using token: a6lgd0.64yqeq0legabnclb
I1015 14:14:26.501235   14117 out.go:204]     ▪ Configuring RBAC rules ...
I1015 14:14:26.501354   14117 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1015 14:14:26.501462   14117 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1015 14:14:26.501705   14117 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1015 14:14:26.501923   14117 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1015 14:14:26.502091   14117 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1015 14:14:26.502377   14117 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1015 14:14:26.502575   14117 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1015 14:14:26.502622   14117 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I1015 14:14:26.502787   14117 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I1015 14:14:26.502811   14117 kubeadm.go:322] 
I1015 14:14:26.502910   14117 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I1015 14:14:26.502927   14117 kubeadm.go:322] 
I1015 14:14:26.503065   14117 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I1015 14:14:26.503080   14117 kubeadm.go:322] 
I1015 14:14:26.503114   14117 kubeadm.go:322]   mkdir -p $HOME/.kube
I1015 14:14:26.503224   14117 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1015 14:14:26.503319   14117 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1015 14:14:26.503323   14117 kubeadm.go:322] 
I1015 14:14:26.503440   14117 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I1015 14:14:26.503458   14117 kubeadm.go:322] 
I1015 14:14:26.503527   14117 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1015 14:14:26.503570   14117 kubeadm.go:322] 
I1015 14:14:26.503683   14117 kubeadm.go:322] You should now deploy a pod network to the cluster.
I1015 14:14:26.503800   14117 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1015 14:14:26.503881   14117 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1015 14:14:26.503899   14117 kubeadm.go:322] 
I1015 14:14:26.503991   14117 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I1015 14:14:26.504105   14117 kubeadm.go:322] and service account keys on each node and then running the following as root:
I1015 14:14:26.504108   14117 kubeadm.go:322] 
I1015 14:14:26.508987   14117 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token a6lgd0.64yqeq0legabnclb \
I1015 14:14:26.509149   14117 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:f2d1b165ec104dd8c14189a619ba173c53b387f65832b128d300653803a8761e \
I1015 14:14:26.509172   14117 kubeadm.go:322] 	--control-plane 
I1015 14:14:26.509192   14117 kubeadm.go:322] 
I1015 14:14:26.509326   14117 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I1015 14:14:26.509346   14117 kubeadm.go:322] 
I1015 14:14:26.509466   14117 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token a6lgd0.64yqeq0legabnclb \
I1015 14:14:26.509641   14117 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:f2d1b165ec104dd8c14189a619ba173c53b387f65832b128d300653803a8761e 
I1015 14:14:26.509660   14117 cni.go:84] Creating CNI manager for ""
I1015 14:14:26.509672   14117 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1015 14:14:26.511965   14117 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1015 14:14:26.514124   14117 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1015 14:14:26.869320   14117 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1015 14:14:27.461018   14117 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1015 14:14:27.461086   14117 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.4/kubectl label nodes minikube.k8s.io/version=v1.31.2 minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_10_15T14_14_27_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1015 14:14:27.461093   14117 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.4/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1015 14:14:27.761672   14117 ops.go:34] apiserver oom_adj: -16
I1015 14:14:29.273479   14117 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.27.4/kubectl label nodes minikube.k8s.io/version=v1.31.2 minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2023_10_15T14_14_27_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig: (1.812351748s)
I1015 14:14:29.273511   14117 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.27.4/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (1.812390201s)
I1015 14:14:29.273533   14117 kubeadm.go:1081] duration metric: took 1.812522162s to wait for elevateKubeSystemPrivileges.
I1015 14:14:29.273562   14117 kubeadm.go:406] StartCluster complete in 33.292978711s
I1015 14:14:29.273586   14117 settings.go:142] acquiring lock: {Name:mk3e20c526bbd9a069c3faffe107e92e911b82b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:14:29.273786   14117 settings.go:150] Updating kubeconfig:  /home/loki/.kube/config
I1015 14:14:29.279190   14117 lock.go:35] WriteFile acquiring /home/loki/.kube/config: {Name:mk755d8845b448654e670abd0d03b77564bbe150 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1015 14:14:29.279499   14117 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1015 14:14:29.279561   14117 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1015 14:14:29.279655   14117 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1015 14:14:29.279672   14117 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1015 14:14:29.279717   14117 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1015 14:14:29.279731   14117 host.go:66] Checking if "minikube" exists ...
I1015 14:14:29.279742   14117 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1015 14:14:29.279751   14117 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1015 14:14:29.280013   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 14:14:29.280349   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 14:14:29.473610   14117 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1015 14:14:29.473655   14117 host.go:66] Checking if "minikube" exists ...
I1015 14:14:29.478964   14117 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1015 14:14:29.493214   14117 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1015 14:14:29.509109   14117 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1015 14:14:29.509123   14117 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1015 14:14:29.509218   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:14:29.663305   14117 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1015 14:14:29.663353   14117 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1015 14:14:29.682863   14117 out.go:177] 🔎  Verifying Kubernetes components...
I1015 14:14:29.700109   14117 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1015 14:14:29.730872   14117 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1015 14:14:29.730894   14117 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1015 14:14:29.731010   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1015 14:14:29.770756   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:14:29.953956   14117 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57059 SSHKeyPath:/home/loki/.minikube/machines/minikube/id_rsa Username:docker}
I1015 14:14:29.990762   14117 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1015 14:14:29.990762   14117 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1015 14:14:30.233416   14117 api_server.go:52] waiting for apiserver process to appear ...
I1015 14:14:30.233469   14117 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1015 14:14:30.279812   14117 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1015 14:14:31.059645   14117 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1015 14:14:37.668951   14117 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (7.678150878s)
I1015 14:14:37.668984   14117 start.go:901] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1015 14:14:37.668990   14117 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (7.435505391s)
I1015 14:14:37.669012   14117 api_server.go:72] duration metric: took 8.005620314s to wait for apiserver process to appear ...
I1015 14:14:37.669019   14117 api_server.go:88] waiting for apiserver healthz status ...
I1015 14:14:37.669042   14117 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57058/healthz ...
I1015 14:14:37.864022   14117 api_server.go:279] https://127.0.0.1:57058/healthz returned 200:
ok
I1015 14:14:37.960721   14117 api_server.go:141] control plane version: v1.27.4
I1015 14:14:37.960756   14117 api_server.go:131] duration metric: took 291.72812ms to wait for apiserver health ...
I1015 14:14:37.960768   14117 system_pods.go:43] waiting for kube-system pods to appear ...
I1015 14:14:38.063939   14117 system_pods.go:59] 4 kube-system pods found
I1015 14:14:38.063958   14117 system_pods.go:61] "etcd-minikube" [b8237879-695c-425b-8a8c-f5ae7c90cf64] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1015 14:14:38.063964   14117 system_pods.go:61] "kube-apiserver-minikube" [b0c9f5fc-bc8b-457e-85af-20eecc6b1bf6] Running
I1015 14:14:38.063967   14117 system_pods.go:61] "kube-controller-manager-minikube" [306f29c5-1d30-4bc8-b9f1-53a920e41a61] Running
I1015 14:14:38.063973   14117 system_pods.go:61] "kube-scheduler-minikube" [d95528b2-38d4-4678-9256-0eb505c06bc0] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1015 14:14:38.063978   14117 system_pods.go:74] duration metric: took 103.204457ms to wait for pod list to return data ...
I1015 14:14:38.063987   14117 kubeadm.go:581] duration metric: took 8.400598396s to wait for : map[apiserver:true system_pods:true] ...
I1015 14:14:38.063998   14117 node_conditions.go:102] verifying NodePressure condition ...
I1015 14:14:38.260791   14117 node_conditions.go:122] node storage ephemeral capacity is 263112772Ki
I1015 14:14:38.260806   14117 node_conditions.go:123] node cpu capacity is 16
I1015 14:14:38.260815   14117 node_conditions.go:105] duration metric: took 196.814803ms to run NodePressure ...
I1015 14:14:38.260826   14117 start.go:228] waiting for startup goroutines ...
I1015 14:14:39.383916   14117 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (9.104079734s)
I1015 14:14:39.383984   14117 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (8.32432725s)
I1015 14:14:39.390875   14117 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1015 14:14:39.403281   14117 addons.go:502] enable addons completed in 10.12372802s: enabled=[storage-provisioner default-storageclass]
I1015 14:14:39.403322   14117 start.go:233] waiting for cluster config update ...
I1015 14:14:39.403333   14117 start.go:242] writing updated cluster config ...
I1015 14:14:39.403621   14117 ssh_runner.go:195] Run: rm -f paused
I1015 14:14:39.579134   14117 start.go:600] kubectl: 1.27.2, cluster: 1.27.4 (minor skew: 0)
I1015 14:14:39.582959   14117 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 15 09:18:11 minikube dockerd[1200]: time="2023-10-15T09:18:11.710062058Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 09:18:39 minikube dockerd[1200]: time="2023-10-15T09:18:39.076034821Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 09:18:39 minikube dockerd[1200]: time="2023-10-15T09:18:39.076170402Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 09:19:35 minikube dockerd[1200]: time="2023-10-15T09:19:35.087563199Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 09:19:35 minikube dockerd[1200]: time="2023-10-15T09:19:35.087654725Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 09:19:36 minikube dockerd[1200]: time="2023-10-15T09:19:36.297041667Z" level=info msg="ignoring event" container=9f1e550b906e13e2a342119111a2374ccd86626f66179f0505a50a7f7a44b849 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:19:54 minikube cri-dockerd[1428]: time="2023-10-15T09:19:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b6f3db8da4686899103605c37aa3ccad7c0da5393bec2b0901326c18daf95960/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:19:57 minikube dockerd[1200]: time="2023-10-15T09:19:57.824600114Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 09:19:57 minikube dockerd[1200]: time="2023-10-15T09:19:57.824696466Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 09:20:05 minikube dockerd[1200]: time="2023-10-15T09:20:05.381613441Z" level=info msg="ignoring event" container=b6f3db8da4686899103605c37aa3ccad7c0da5393bec2b0901326c18daf95960 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:20:56 minikube cri-dockerd[1428]: time="2023-10-15T09:20:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ba8c3013e59e8e8d2e3041b3a4b15c0ef333d0396f5975843ac8beb54e13ceda/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:21:22 minikube dockerd[1200]: time="2023-10-15T09:21:22.100301179Z" level=info msg="ignoring event" container=0367b55f9b4b678619bff152f76573ca019db9f427c738de63121850f3d80405 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:21:22 minikube dockerd[1200]: time="2023-10-15T09:21:22.381666409Z" level=info msg="ignoring event" container=ba8c3013e59e8e8d2e3041b3a4b15c0ef333d0396f5975843ac8beb54e13ceda module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:21:29 minikube cri-dockerd[1428]: time="2023-10-15T09:21:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5e261b91632f67dc188c6fc99687055d5b0c2326a8e0051cf95f705254f50d4e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:25:31 minikube dockerd[1200]: time="2023-10-15T09:25:31.966334548Z" level=info msg="ignoring event" container=c9d4c3c8eab2f683b221a1371276a6e36d532581fa9e7868a140b608afb317df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:25:32 minikube dockerd[1200]: time="2023-10-15T09:25:32.269753906Z" level=info msg="ignoring event" container=5e261b91632f67dc188c6fc99687055d5b0c2326a8e0051cf95f705254f50d4e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:25:41 minikube cri-dockerd[1428]: time="2023-10-15T09:25:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c69bdb97957c6785f36bfb269d69d85a972426f97b79101dbdb413d0bd51708a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:25:51 minikube dockerd[1200]: time="2023-10-15T09:25:51.178625951Z" level=info msg="ignoring event" container=c69bdb97957c6785f36bfb269d69d85a972426f97b79101dbdb413d0bd51708a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:26:44 minikube dockerd[1200]: time="2023-10-15T09:26:44.989542189Z" level=info msg="ignoring event" container=29b3288d3c4d973d1a5f1c625c22dc7ddbd2c1d086c1a41c98a91133bcdcd797 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:28:35 minikube dockerd[1200]: time="2023-10-15T09:28:35.550604740Z" level=info msg="ignoring event" container=86136fbb2fcdda7c8a40b14c27746848506321786e5099afb8e236a3a0177aaf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:32:28 minikube cri-dockerd[1428]: time="2023-10-15T09:32:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1b10f09104c881cb18cea2df5f2f7a24bc00a1b60de8f7307a8dead15a99410b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:32:38 minikube dockerd[1200]: time="2023-10-15T09:32:38.820307450Z" level=info msg="ignoring event" container=1b10f09104c881cb18cea2df5f2f7a24bc00a1b60de8f7307a8dead15a99410b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:32:45 minikube cri-dockerd[1428]: time="2023-10-15T09:32:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/497eae4ae87b64094dffb01b01ca001ec9b0fb60795a8cb5cdbfaf8f99397393/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:44:57 minikube dockerd[1200]: time="2023-10-15T09:44:57.173056784Z" level=info msg="ignoring event" container=10663535700ce7e33442368165475eef7fc93f2640f42e07be7df6b2544c341e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:44:57 minikube dockerd[1200]: time="2023-10-15T09:44:57.523433118Z" level=info msg="ignoring event" container=497eae4ae87b64094dffb01b01ca001ec9b0fb60795a8cb5cdbfaf8f99397393 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 09:45:04 minikube cri-dockerd[1428]: time="2023-10-15T09:45:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6e3388b1f8a1ebc16406a85660a0374c870daa28a7628b65c2a6f212ecec8d03/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:45:10 minikube cri-dockerd[1428]: time="2023-10-15T09:45:10Z" level=info msg="Stop pulling image kicbase/echo-server:1.0: Status: Downloaded newer image for kicbase/echo-server:1.0"
Oct 15 09:58:50 minikube cri-dockerd[1428]: time="2023-10-15T09:58:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a1ddc276340bf725fb441418253c02f7ad3fe4c349fcb5808e2a3a36b372749/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:58:50 minikube cri-dockerd[1428]: time="2023-10-15T09:58:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e47ddffaeed7ee61ac27962fee9b4ffe668b274b10efb8ad5b7b8bc1fd38159d/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 09:58:51 minikube dockerd[1200]: time="2023-10-15T09:58:51.910474092Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Oct 15 09:59:10 minikube cri-dockerd[1428]: time="2023-10-15T09:59:10Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Downloading [===========================>                       ]  11.05MB/19.74MB"
Oct 15 09:59:14 minikube cri-dockerd[1428]: time="2023-10-15T09:59:14Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Oct 15 09:59:15 minikube dockerd[1200]: time="2023-10-15T09:59:15.247163903Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Oct 15 09:59:27 minikube cri-dockerd[1428]: time="2023-10-15T09:59:27Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [===>                                               ]  5.929MB/75.78MB"
Oct 15 09:59:37 minikube cri-dockerd[1428]: time="2023-10-15T09:59:37Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [=================>                                 ]  26.47MB/75.78MB"
Oct 15 09:59:47 minikube cri-dockerd[1428]: time="2023-10-15T09:59:47Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [=====================>                             ]  31.87MB/75.78MB"
Oct 15 09:59:57 minikube cri-dockerd[1428]: time="2023-10-15T09:59:57Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [=========================>                         ]  38.34MB/75.78MB"
Oct 15 10:00:07 minikube cri-dockerd[1428]: time="2023-10-15T10:00:07Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [=========================================>         ]  62.66MB/75.78MB"
Oct 15 10:00:17 minikube cri-dockerd[1428]: time="2023-10-15T10:00:17Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [=================================================> ]  74.65MB/75.78MB"
Oct 15 10:00:17 minikube cri-dockerd[1428]: time="2023-10-15T10:00:17Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Oct 15 10:06:02 minikube dockerd[1200]: time="2023-10-15T10:06:02.224717507Z" level=info msg="ignoring event" container=d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 10:06:02 minikube dockerd[1200]: time="2023-10-15T10:06:02.589769339Z" level=info msg="ignoring event" container=6e3388b1f8a1ebc16406a85660a0374c870daa28a7628b65c2a6f212ecec8d03 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 10:06:03 minikube cri-dockerd[1428]: time="2023-10-15T10:06:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1605fc22aae704b8268f946bbfee439bc995af9fa6f1a9f8496a5c4208f8cdec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 10:06:24 minikube cri-dockerd[1428]: time="2023-10-15T10:06:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6d48c95a7c454e645251725c96fac870cccd16542a2750279ed8e51442c2b015/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 10:06:28 minikube dockerd[1200]: time="2023-10-15T10:06:28.600922441Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 10:06:28 minikube dockerd[1200]: time="2023-10-15T10:06:28.600985994Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 10:06:46 minikube dockerd[1200]: time="2023-10-15T10:06:46.478016280Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 10:06:46 minikube dockerd[1200]: time="2023-10-15T10:06:46.478104845Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 10:07:17 minikube dockerd[1200]: time="2023-10-15T10:07:17.801615860Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 10:07:17 minikube dockerd[1200]: time="2023-10-15T10:07:17.802037021Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 10:08:10 minikube dockerd[1200]: time="2023-10-15T10:08:10.730492709Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 15 10:08:10 minikube dockerd[1200]: time="2023-10-15T10:08:10.730550205Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 15 10:09:24 minikube dockerd[1200]: time="2023-10-15T10:09:24.005459955Z" level=info msg="ignoring event" container=6d48c95a7c454e645251725c96fac870cccd16542a2750279ed8e51442c2b015 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 10:09:39 minikube dockerd[1200]: time="2023-10-15T10:09:39.931446818Z" level=info msg="ignoring event" container=b2bc16b6a7d05f8a8edeb2bd356b4b7f0436b220eef1b49bf37d002e1fee5733 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 10:09:40 minikube dockerd[1200]: time="2023-10-15T10:09:40.248566135Z" level=info msg="ignoring event" container=1605fc22aae704b8268f946bbfee439bc995af9fa6f1a9f8496a5c4208f8cdec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 15 10:09:49 minikube cri-dockerd[1428]: time="2023-10-15T10:09:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/21dab93651e176ae42fb772ec3fc2b918b28d1859554759ad860e0a2f87c844d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 15 10:10:04 minikube cri-dockerd[1428]: time="2023-10-15T10:10:04Z" level=info msg="Pulling image gcr.io/google-samples/hello-app:1.0: f29249862b9e: Downloading [==========>                                        ]  831.7kB/3.888MB"
Oct 15 10:10:14 minikube cri-dockerd[1428]: time="2023-10-15T10:10:14Z" level=info msg="Pulling image gcr.io/google-samples/hello-app:1.0: f29249862b9e: Downloading [===================================>               ]  2.752MB/3.888MB"
Oct 15 10:10:24 minikube cri-dockerd[1428]: time="2023-10-15T10:10:24Z" level=info msg="Pulling image gcr.io/google-samples/hello-app:1.0: 96266735468f: Downloading [===================================>               ]  4.098MB/5.845MB"
Oct 15 10:10:34 minikube cri-dockerd[1428]: time="2023-10-15T10:10:34Z" level=info msg="Pulling image gcr.io/google-samples/hello-app:1.0: 96266735468f: Downloading [===========================================>       ]  5.081MB/5.845MB"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
305f9f3f0be8a       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         10 minutes ago      Running             kubernetes-dashboard        0                   e47ddffaeed7e       kubernetes-dashboard-5c5cfc8747-rr6dl
3b5c62b931dcb       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   11 minutes ago      Running             dashboard-metrics-scraper   0                   7a1ddc276340b       dashboard-metrics-scraper-5dd9cbfd69-bfrmt
f1702b202a48d       6e38f40d628db                                                                                          3 hours ago         Running             storage-provisioner         1                   6ea8098d2ddd4       storage-provisioner
f8557806da708       ead0a4a53df89                                                                                          3 hours ago         Running             coredns                     0                   1f2d29643ecc0       coredns-5d78c9869d-bdbxs
97886c62076cd       6e38f40d628db                                                                                          3 hours ago         Exited              storage-provisioner         0                   6ea8098d2ddd4       storage-provisioner
edc8d4a5109e0       6848d7eda0341                                                                                          3 hours ago         Running             kube-proxy                  0                   26d52e41a5e49       kube-proxy-df8h6
c8afa0499bc03       e7972205b6614                                                                                          3 hours ago         Running             kube-apiserver              0                   8cc97630f1633       kube-apiserver-minikube
a5108728b7a03       f466468864b7a                                                                                          3 hours ago         Running             kube-controller-manager     0                   a68459efc8b80       kube-controller-manager-minikube
1c1b5cfb763ad       86b6af7dd652c                                                                                          3 hours ago         Running             etcd                        0                   132b42193e2ff       etcd-minikube
02d0aa47fd0a9       98ef2570f3cde                                                                                          3 hours ago         Running             kube-scheduler              0                   c88d4c94e52c7       kube-scheduler-minikube


==> coredns [f8557806da70] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:37414 - 24769 "HINFO IN 9140356809563869413.652981361291235873. udp 56 false 512" NXDOMAIN qr,rd,ra 56 1.271598891s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_10_15T14_14_27_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 15 Oct 2023 07:14:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 15 Oct 2023 10:10:26 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 15 Oct 2023 10:05:33 +0000   Sun, 15 Oct 2023 07:14:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 15 Oct 2023 10:05:33 +0000   Sun, 15 Oct 2023 07:14:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 15 Oct 2023 10:05:33 +0000   Sun, 15 Oct 2023 07:14:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 15 Oct 2023 10:05:33 +0000   Sun, 15 Oct 2023 07:14:38 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32262420Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32262420Ki
  pods:               110
System Info:
  Machine ID:                 0594ad9baf9b40e6831e1336ef032ab0
  System UUID:                0594ad9baf9b40e6831e1336ef032ab0
  Boot ID:                    829eb179-dde9-42c2-8055-3843fb78c613
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     web-548f6458b5-jlcr5                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47s
  kube-system                 coredns-5d78c9869d-bdbxs                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     175m
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         176m
  kube-system                 kube-apiserver-minikube                       250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         176m
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         176m
  kube-system                 kube-proxy-df8h6                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         175m
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         176m
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         175m
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-bfrmt    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-rr6dl         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (0%!)(MISSING)  170Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Oct15 01:35] PCI: Fatal: No config space access function found
[  +0.019091] PCI: System does not support PCI
[  +0.018830] kvm: no hardware support
[  +0.000003] kvm: no hardware support
[  +1.283024] FS-Cache: Duplicate cookie detected
[  +0.000755] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000649] FS-Cache: O-cookie d=00000000515cb17d{9P.session} n=000000001be9c7db
[  +0.001000] FS-Cache: O-key=[10] '34323934393337343334'
[  +0.000462] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000658] FS-Cache: N-cookie d=00000000515cb17d{9P.session} n=00000000be8ad6bd
[  +0.000717] FS-Cache: N-key=[10] '34323934393337343334'
[  +0.002517] FS-Cache: Duplicate cookie detected
[  +0.000500] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000502] FS-Cache: O-cookie d=00000000515cb17d{9P.session} n=000000001be9c7db
[  +0.000579] FS-Cache: O-key=[10] '34323934393337343334'
[  +0.000373] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000412] FS-Cache: N-cookie d=00000000515cb17d{9P.session} n=00000000ea542e2e
[  +0.000521] FS-Cache: N-key=[10] '34323934393337343334'
[ +21.231664] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000517] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000391] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000506] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +4.759422] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.015317] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Saigon not found. Is the tzdata package installed?
[  +0.091034] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000532] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000382] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000554] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.724769] WSL (2) ERROR: UtilCreateProcessAndWait:662: /bin/mount failed with 2
[  +0.000865] WSL (1) ERROR: UtilCreateProcessAndWait:684: /bin/mount failed with status 0xff00

[  +0.001124] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.002706] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.017983] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Saigon not found. Is the tzdata package installed?
[  +0.107131] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000705] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000643] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000842] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Oct15 01:55] CPU: 12 PID: 14397 Comm: nvim Not tainted 5.15.90.1-microsoft-standard-WSL2 #1
[  +0.000772] RIP: 0033:0x7f0bb799f00b
[  +0.000281] Code: c2 b8 ea 00 00 00 0f 05 48 3d 00 f0 ff ff 77 3f 41 89 c0 41 ba 08 00 00 00 31 d2 4c 89 ce bf 02 00 00 00 b8 0e 00 00 00 0f 05 <48> 8b 84 24 08 01 00 00 64 48 33 04 25 28 00 00 00 75 26 44 89 c0
[  +0.001368] RSP: 002b:00007fff9eb3bc30 EFLAGS: 00000246 ORIG_RAX: 000000000000000e
[  +0.000705] RAX: 0000000000000000 RBX: 00007f0bb7952740 RCX: 00007f0bb799f00b
[  +0.000459] RDX: 0000000000000000 RSI: 00007fff9eb3bc30 RDI: 0000000000000002
[  +0.000514] RBP: 00007f0bb7b14588 R08: 0000000000000000 R09: 00007fff9eb3bc30
[  +0.000503] R10: 0000000000000008 R11: 0000000000000246 R12: 000056380db05ef0
[  +0.000373] R13: 0000000000000075 R14: 000056380db05f29 R15: 0000000000000000
[  +0.000378] FS:  00007f0bb7952740 GS:  0000000000000000


==> etcd [1c1b5cfb763a] <==
{"level":"warn","ts":"2023-10-15T09:05:17.664Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"330.3782ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-15T09:05:17.664Z","caller":"traceutil/trace.go:171","msg":"trace[931575451] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:4305; }","duration":"330.452729ms","start":"2023-10-15T08:52:50.011Z","end":"2023-10-15T09:05:17.664Z","steps":["trace[931575451] 'agreement among raft nodes before linearized reading'  (duration: 330.346541ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-15T09:05:17.664Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-15T08:52:50.011Z","time spent":"330.505226ms","remote":"127.0.0.1:56986","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":29,"request content":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true "}
{"level":"warn","ts":"2023-10-15T09:05:17.664Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"330.930268ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-15T09:05:17.664Z","caller":"traceutil/trace.go:171","msg":"trace[1785811900] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:4305; }","duration":"330.981013ms","start":"2023-10-15T08:52:50.011Z","end":"2023-10-15T09:05:17.664Z","steps":["trace[1785811900] 'agreement among raft nodes before linearized reading'  (duration: 330.90437ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-15T09:05:17.664Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-15T08:52:50.011Z","time spent":"331.035495ms","remote":"127.0.0.1:56570","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":29,"request content":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true "}
{"level":"warn","ts":"2023-10-15T09:05:17.664Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.176046ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2023-10-15T09:05:17.664Z","caller":"traceutil/trace.go:171","msg":"trace[379126072] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:4305; }","duration":"193.22668ms","start":"2023-10-15T09:05:17.471Z","end":"2023-10-15T09:05:17.664Z","steps":["trace[379126072] 'agreement among raft nodes before linearized reading'  (duration: 193.136352ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-15T09:05:17.750Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.61673ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:339"}
{"level":"info","ts":"2023-10-15T09:05:17.750Z","caller":"traceutil/trace.go:171","msg":"trace[917632588] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:4305; }","duration":"198.778962ms","start":"2023-10-15T09:05:17.551Z","end":"2023-10-15T09:05:17.750Z","steps":["trace[917632588] 'agreement among raft nodes before linearized reading'  (duration: 198.468845ms)"],"step_count":1}
{"level":"info","ts":"2023-10-15T09:05:17.860Z","caller":"traceutil/trace.go:171","msg":"trace[1938244996] transaction","detail":"{read_only:false; response_revision:4306; number_of_response:1; }","duration":"104.194571ms","start":"2023-10-15T09:05:17.756Z","end":"2023-10-15T09:05:17.860Z","steps":["trace[1938244996] 'process raft request'  (duration: 103.912215ms)"],"step_count":1}
{"level":"info","ts":"2023-10-15T09:09:47.670Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4296}
{"level":"info","ts":"2023-10-15T09:09:47.672Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4296,"took":"810.353µs","hash":951145704}
{"level":"info","ts":"2023-10-15T09:09:47.672Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":951145704,"revision":4296,"compact-revision":4052}
{"level":"info","ts":"2023-10-15T09:11:15.029Z","caller":"traceutil/trace.go:171","msg":"trace[829724157] linearizableReadLoop","detail":"{readStateIndex:5759; appliedIndex:5758; }","duration":"433.013752ms","start":"2023-10-15T09:11:14.596Z","end":"2023-10-15T09:11:15.029Z","steps":["trace[829724157] 'read index received'  (duration: 432.859364ms)","trace[829724157] 'applied index is now lower than readState.Index'  (duration: 153.696µs)"],"step_count":2}
{"level":"info","ts":"2023-10-15T09:11:15.029Z","caller":"traceutil/trace.go:171","msg":"trace[81984431] transaction","detail":"{read_only:false; response_revision:4627; number_of_response:1; }","duration":"602.697204ms","start":"2023-10-15T09:11:14.426Z","end":"2023-10-15T09:11:15.029Z","steps":["trace[81984431] 'process raft request'  (duration: 602.406191ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-15T09:11:15.029Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"433.211371ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2023-10-15T09:11:15.029Z","caller":"traceutil/trace.go:171","msg":"trace[1691594680] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:4627; }","duration":"433.256856ms","start":"2023-10-15T09:11:14.596Z","end":"2023-10-15T09:11:15.029Z","steps":["trace[1691594680] 'agreement among raft nodes before linearized reading'  (duration: 433.148473ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-15T09:11:15.029Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-15T09:11:14.596Z","time spent":"433.298474ms","remote":"127.0.0.1:56646","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":625,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-10-15T09:11:15.029Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-15T09:11:14.426Z","time spent":"602.774579ms","remote":"127.0.0.1:56778","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:4619 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2023-10-15T09:14:47.691Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4545}
{"level":"info","ts":"2023-10-15T09:14:47.693Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4545,"took":"1.08168ms","hash":2750240749}
{"level":"info","ts":"2023-10-15T09:14:47.693Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2750240749,"revision":4545,"compact-revision":4296}
{"level":"info","ts":"2023-10-15T09:16:03.507Z","caller":"traceutil/trace.go:171","msg":"trace[2079587032] linearizableReadLoop","detail":"{readStateIndex:6050; appliedIndex:6049; }","duration":"162.127333ms","start":"2023-10-15T09:16:03.345Z","end":"2023-10-15T09:16:03.507Z","steps":["trace[2079587032] 'read index received'  (duration: 161.717914ms)","trace[2079587032] 'applied index is now lower than readState.Index'  (duration: 408.858µs)"],"step_count":2}
{"level":"info","ts":"2023-10-15T09:16:03.507Z","caller":"traceutil/trace.go:171","msg":"trace[179893233] transaction","detail":"{read_only:false; response_revision:4859; number_of_response:1; }","duration":"254.614774ms","start":"2023-10-15T09:16:03.252Z","end":"2023-10-15T09:16:03.507Z","steps":["trace[179893233] 'process raft request'  (duration: 254.053677ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-15T09:16:03.507Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.320737ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-10-15T09:16:03.507Z","caller":"traceutil/trace.go:171","msg":"trace[1320490229] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4859; }","duration":"162.369027ms","start":"2023-10-15T09:16:03.345Z","end":"2023-10-15T09:16:03.507Z","steps":["trace[1320490229] 'agreement among raft nodes before linearized reading'  (duration: 162.259501ms)"],"step_count":1}
{"level":"info","ts":"2023-10-15T09:19:47.710Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4798}
{"level":"info","ts":"2023-10-15T09:19:47.711Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4798,"took":"690.357µs","hash":1809755553}
{"level":"info","ts":"2023-10-15T09:19:47.711Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1809755553,"revision":4798,"compact-revision":4545}
{"level":"info","ts":"2023-10-15T09:24:47.722Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5078}
{"level":"info","ts":"2023-10-15T09:24:47.723Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5078,"took":"730.027µs","hash":1604254173}
{"level":"info","ts":"2023-10-15T09:24:47.723Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1604254173,"revision":5078,"compact-revision":4798}
{"level":"info","ts":"2023-10-15T09:29:47.711Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5355}
{"level":"info","ts":"2023-10-15T09:29:47.712Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5355,"took":"813.629µs","hash":3008950054}
{"level":"info","ts":"2023-10-15T09:29:47.712Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3008950054,"revision":5355,"compact-revision":5078}
{"level":"info","ts":"2023-10-15T09:34:47.734Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5616}
{"level":"info","ts":"2023-10-15T09:34:47.735Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5616,"took":"821.101µs","hash":1727213903}
{"level":"info","ts":"2023-10-15T09:34:47.735Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1727213903,"revision":5616,"compact-revision":5355}
{"level":"info","ts":"2023-10-15T09:39:47.763Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5880}
{"level":"info","ts":"2023-10-15T09:39:47.764Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5880,"took":"535.298µs","hash":3667325297}
{"level":"info","ts":"2023-10-15T09:39:47.764Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3667325297,"revision":5880,"compact-revision":5616}
{"level":"info","ts":"2023-10-15T09:44:47.794Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6119}
{"level":"info","ts":"2023-10-15T09:44:47.796Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6119,"took":"687.131µs","hash":3836215151}
{"level":"info","ts":"2023-10-15T09:44:47.796Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3836215151,"revision":6119,"compact-revision":5880}
{"level":"info","ts":"2023-10-15T09:49:47.800Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6361}
{"level":"info","ts":"2023-10-15T09:49:47.801Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6361,"took":"731.397µs","hash":3123842896}
{"level":"info","ts":"2023-10-15T09:49:47.801Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3123842896,"revision":6361,"compact-revision":6119}
{"level":"info","ts":"2023-10-15T09:54:47.824Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6636}
{"level":"info","ts":"2023-10-15T09:54:47.825Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6636,"took":"806.463µs","hash":943485709}
{"level":"info","ts":"2023-10-15T09:54:47.825Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":943485709,"revision":6636,"compact-revision":6361}
{"level":"info","ts":"2023-10-15T09:59:47.849Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6877}
{"level":"info","ts":"2023-10-15T09:59:47.850Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6877,"took":"699.911µs","hash":2441636561}
{"level":"info","ts":"2023-10-15T09:59:47.850Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2441636561,"revision":6877,"compact-revision":6636}
{"level":"info","ts":"2023-10-15T10:04:47.876Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7185}
{"level":"info","ts":"2023-10-15T10:04:47.877Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7185,"took":"877.099µs","hash":3674654015}
{"level":"info","ts":"2023-10-15T10:04:47.877Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3674654015,"revision":7185,"compact-revision":6877}
{"level":"info","ts":"2023-10-15T10:09:47.888Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7434}
{"level":"info","ts":"2023-10-15T10:09:47.890Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7434,"took":"814.386µs","hash":1364297647}
{"level":"info","ts":"2023-10-15T10:09:47.890Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1364297647,"revision":7434,"compact-revision":7185}


==> kernel <==
 10:10:35 up  8:35,  0 users,  load average: 0.92, 0.77, 0.71
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"


==> kube-apiserver [c8afa0499bc0] <==
Trace[693866776]:  ---"About to Encode" 199ms (08:22:10.733)
Trace[693866776]:  ---"Txn call completed" 1200ms (08:22:11.933)]
Trace[693866776]: ---"About to check admission control" 100ms (08:22:10.633)
Trace[693866776]: ---"Object stored in database" 1300ms (08:22:11.933)
Trace[693866776]: [1.700204934s] [1.700204934s] END
I1015 08:22:12.443688       1 trace.go:219] Trace[590994810]: "Get" accept:application/json, */*,audit-id:e34202d9-3c66-4470-a0b4-0ea171243664,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (15-Oct-2023 08:22:10.943) (total time: 1499ms):
Trace[590994810]: ---"About to write a response" 1190ms (08:22:12.133)
Trace[590994810]: ---"Writing http response done" 309ms (08:22:12.443)
Trace[590994810]: [1.499759031s] [1.499759031s] END
I1015 08:22:13.638759       1 trace.go:219] Trace[120709684]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b70adcf5-4f90-4929-9de3-d2503d32c7be,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/default/pods/kubia/status,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PATCH (15-Oct-2023 08:22:11.034) (total time: 2599ms):
Trace[120709684]: ---"limitedReadBody succeeded" len:122 99ms (08:22:11.133)
Trace[120709684]: ["GuaranteedUpdate etcd3" audit-id:b70adcf5-4f90-4929-9de3-d2503d32c7be,key:/pods/default/kubia,type:*core.Pod,resource:pods 2499ms (08:22:11.133)
Trace[120709684]:  ---"About to Encode" 1199ms (08:22:12.333)
Trace[120709684]:  ---"Txn call completed" 1009ms (08:22:13.343)]
Trace[120709684]: ---"About to check admission control" 1100ms (08:22:12.234)
Trace[120709684]: ---"Object stored in database" 1110ms (08:22:13.344)
Trace[120709684]: ---"Writing http response done" 289ms (08:22:13.638)
Trace[120709684]: [2.599604716s] [2.599604716s] END
I1015 08:22:14.339067       1 trace.go:219] Trace[598258060]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:cd256a58-5124-485f-a792-87a0a98876de,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (15-Oct-2023 08:22:13.044) (total time: 1289ms):
Trace[598258060]: ---"About to convert to expected version" 89ms (08:22:13.133)
Trace[598258060]: ["Create etcd3" audit-id:cd256a58-5124-485f-a792-87a0a98876de,key:/events/default/kubia.178e3a39ffc43c74,type:*core.Event,resource:events 1100ms (08:22:13.233)
Trace[598258060]:  ---"Txn call succeeded" 1099ms (08:22:14.338)]
Trace[598258060]: [1.289956962s] [1.289956962s] END
I1015 08:22:15.448761       1 trace.go:219] Trace[1712118283]: "Update" accept:application/json, */*,audit-id:9ce7805e-69de-4e89-8c84-f7a8bffa5f69,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (15-Oct-2023 08:22:13.538) (total time: 1909ms):
Trace[1712118283]: ["GuaranteedUpdate etcd3" audit-id:9ce7805e-69de-4e89-8c84-f7a8bffa5f69,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1900ms (08:22:13.548)
Trace[1712118283]:  ---"About to Encode" 90ms (08:22:13.638)
Trace[1712118283]:  ---"Txn call completed" 1809ms (08:22:15.448)]
Trace[1712118283]: [1.909910035s] [1.909910035s] END
I1015 08:22:30.277578       1 trace.go:219] Trace[755959893]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e5a94c84-259e-4ae5-9122-0fd2acc7f058,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (15-Oct-2023 08:22:14.638) (total time: 15638ms):
Trace[755959893]: ---"About to store object in database" 99ms (08:22:14.738)
Trace[755959893]: ["GuaranteedUpdate etcd3" audit-id:e5a94c84-259e-4ae5-9122-0fd2acc7f058,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 15538ms (08:22:14.738)
Trace[755959893]:  ---"About to Encode" 110ms (08:22:14.848)
Trace[755959893]:  ---"Txn call completed" 15428ms (08:22:30.277)]
Trace[755959893]: [15.638556982s] [15.638556982s] END
E1015 08:22:30.290670       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:"etcdserver: request timed out"}: etcdserver: request timed out
I1015 08:35:37.485106       1 trace.go:219] Trace[338251877]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3a243204-f0b8-4e32-9fe8-988e0a8a85ee,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (15-Oct-2023 08:22:15.238) (total time: 15095ms):
Trace[338251877]: ["Create etcd3" audit-id:3a243204-f0b8-4e32-9fe8-988e0a8a85ee,key:/events/default/kubia.178e3a39ffc49918,type:*core.Event,resource:events 14885ms (08:22:15.448)
Trace[338251877]:  ---"Txn call failed" err:etcdserver: request timed out 14841ms (08:22:30.290)]
Trace[338251877]: [15.09511871s] [15.09511871s] END
I1015 08:35:37.986772       1 trace.go:219] Trace[1789712247]: "Get" accept:application/json, */*,audit-id:b5dd62ef-7134-41e9-a54d-930ed1b6a533,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (15-Oct-2023 08:35:37.486) (total time: 500ms):
Trace[1789712247]: ---"About to write a response" 500ms (08:35:37.986)
Trace[1789712247]: [500.234679ms] [500.234679ms] END
I1015 08:35:38.244500       1 trace.go:219] Trace[206695960]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (15-Oct-2023 08:22:12.733) (total time: 18354ms):
Trace[206695960]: ---"initial value restored" 1499ms (08:22:14.238)
Trace[206695960]: ---"Transaction prepared" 16037ms (08:22:30.276)
Trace[206695960]: ---"Txn call completed" 558ms (08:35:37.986)
Trace[206695960]: ---"Transaction prepared" 157ms (08:35:38.144)
Trace[206695960]: ---"Txn call completed" 100ms (08:35:38.244)
Trace[206695960]: [18.354546576s] [18.354546576s] END
E1015 09:05:47.489395       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1015 09:05:47.566373       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I1015 09:11:15.030690       1 trace.go:219] Trace[1202287839]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:96151ec2-6ad9-4d3f-9ba6-1ab6c309d980,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (15-Oct-2023 09:11:14.425) (total time: 605ms):
Trace[1202287839]: ["GuaranteedUpdate etcd3" audit-id:96151ec2-6ad9-4d3f-9ba6-1ab6c309d980,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 605ms (09:11:14.425)
Trace[1202287839]:  ---"Txn call completed" 604ms (09:11:15.030)]
Trace[1202287839]: [605.18414ms] [605.18414ms] END
I1015 09:45:03.755155       1 alloc.go:330] "allocated clusterIPs" service="default/hello-minikube1" clusterIPs=map[IPv4:10.99.32.179]
I1015 09:58:49.655601       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs=map[IPv4:10.107.116.132]
I1015 09:58:49.741128       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs=map[IPv4:10.100.19.141]
I1015 10:06:41.297970       1 alloc.go:330] "allocated clusterIPs" service="default/app-b" clusterIPs=map[IPv4:10.102.239.138]
I1015 10:09:52.289497       1 alloc.go:330] "allocated clusterIPs" service="default/web" clusterIPs=map[IPv4:10.106.2.114]


==> kube-controller-manager [a5108728b7a0] <==
I1015 07:14:40.653999       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1015 07:14:40.654056       1 shared_informer.go:318] Caches are synced for daemon sets
I1015 07:14:40.654078       1 shared_informer.go:318] Caches are synced for job
I1015 07:14:40.659126       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1015 07:14:40.659662       1 shared_informer.go:318] Caches are synced for taint
I1015 07:14:40.659770       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1015 07:14:40.659935       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1015 07:14:40.659995       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1015 07:14:40.660015       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1015 07:14:40.660042       1 taint_manager.go:211] "Sending events to api server"
I1015 07:14:40.660757       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1015 07:14:40.660808       1 shared_informer.go:318] Caches are synced for endpoint
I1015 07:14:40.761976       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=[10.244.0.0/24]
I1015 07:14:40.763883       1 shared_informer.go:318] Caches are synced for disruption
I1015 07:14:40.769820       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1015 07:14:40.772725       1 shared_informer.go:318] Caches are synced for PVC protection
I1015 07:14:40.853739       1 shared_informer.go:318] Caches are synced for stateful set
I1015 07:14:40.853804       1 shared_informer.go:318] Caches are synced for expand
I1015 07:14:40.854051       1 shared_informer.go:318] Caches are synced for attach detach
I1015 07:14:40.854114       1 shared_informer.go:318] Caches are synced for resource quota
I1015 07:14:40.854113       1 shared_informer.go:318] Caches are synced for ReplicationController
I1015 07:14:40.858931       1 shared_informer.go:318] Caches are synced for ephemeral
I1015 07:14:40.859428       1 shared_informer.go:318] Caches are synced for PV protection
I1015 07:14:40.859888       1 shared_informer.go:318] Caches are synced for persistent volume
I1015 07:14:40.862464       1 shared_informer.go:318] Caches are synced for resource quota
I1015 07:14:41.155475       1 shared_informer.go:318] Caches are synced for garbage collector
I1015 07:14:41.165435       1 shared_informer.go:318] Caches are synced for garbage collector
I1015 07:14:41.165473       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1015 07:14:41.461997       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5d78c9869d to 1"
I1015 07:14:41.661916       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-df8h6"
I1015 07:14:42.161515       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-bdbxs"
I1015 07:14:42.555425       1 event.go:307] "Event occurred" object="kube-dns" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToCreateEndpoint" message="Failed to create endpoint for service kube-system/kube-dns: endpoints \"kube-dns\" already exists"
W1015 09:05:47.489842       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E1015 09:05:47.566835       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
I1015 09:40:15.500608       1 cleaner.go:172] Cleaning CSR "csr-s688l" as it is more than 1h0m0s old and approved.
I1015 09:45:03.640090       1 event.go:307] "Event occurred" object="default/hello-minikube1" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set hello-minikube1-5c5f84495c to 1"
I1015 09:45:03.666239       1 event.go:307] "Event occurred" object="default/hello-minikube1-5c5f84495c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hello-minikube1-5c5f84495c-xmfjn"
I1015 09:58:49.347083       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-5dd9cbfd69 to 1"
I1015 09:58:49.358778       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E1015 09:58:49.367953       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I1015 09:58:49.368098       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-5c5cfc8747 to 1"
E1015 09:58:49.409423       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I1015 09:58:49.410352       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I1015 09:58:49.411989       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E1015 09:58:49.416418       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I1015 09:58:49.416433       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E1015 09:58:49.424895       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
E1015 09:58:49.427563       1 replica_set.go:544] sync "kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" failed with pods "dashboard-metrics-scraper-5dd9cbfd69-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I1015 09:58:49.427593       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-5dd9cbfd69-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E1015 09:58:49.430940       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I1015 09:58:49.430952       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
E1015 09:58:49.435210       1 replica_set.go:544] sync "kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" failed with pods "kubernetes-dashboard-5c5cfc8747-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I1015 09:58:49.435231       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-5c5cfc8747-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I1015 09:58:49.530344       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-5c5cfc8747-rr6dl"
I1015 09:58:49.530396       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-5dd9cbfd69-bfrmt"
I1015 10:06:02.109305       1 event.go:307] "Event occurred" object="default/hello-minikube1-5c5f84495c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hello-minikube1-5c5f84495c-s5jrk"
I1015 10:06:24.084417       1 event.go:307] "Event occurred" object="default/app-b" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app-b-9f6f7b4cf to 1"
I1015 10:06:24.095200       1 event.go:307] "Event occurred" object="default/app-b-9f6f7b4cf" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-b-9f6f7b4cf-ngqr7"
I1015 10:09:48.703137       1 event.go:307] "Event occurred" object="default/web" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set web-548f6458b5 to 1"
I1015 10:09:48.718673       1 event.go:307] "Event occurred" object="default/web-548f6458b5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: web-548f6458b5-jlcr5"


==> kube-proxy [edc8d4a5109e] <==
I1015 07:14:48.263109       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1015 07:14:48.263196       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1015 07:14:48.263232       1 server_others.go:554] "Using iptables proxy"
I1015 07:14:48.863882       1 server_others.go:192] "Using iptables Proxier"
I1015 07:14:48.863939       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1015 07:14:48.863955       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1015 07:14:48.863977       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1015 07:14:48.864025       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1015 07:14:48.865108       1 server.go:658] "Version info" version="v1.27.4"
I1015 07:14:48.865148       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1015 07:14:48.870522       1 config.go:188] "Starting service config controller"
I1015 07:14:48.870571       1 shared_informer.go:311] Waiting for caches to sync for service config
I1015 07:14:48.870613       1 config.go:97] "Starting endpoint slice config controller"
I1015 07:14:48.870646       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1015 07:14:48.871010       1 config.go:315] "Starting node config controller"
I1015 07:14:48.871049       1 shared_informer.go:311] Waiting for caches to sync for node config
I1015 07:14:49.055346       1 shared_informer.go:318] Caches are synced for node config
I1015 07:14:49.055351       1 shared_informer.go:318] Caches are synced for service config
I1015 07:14:49.055368       1 shared_informer.go:318] Caches are synced for endpoint slice config


==> kube-scheduler [02d0aa47fd0a] <==
E1015 07:14:17.861543       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1015 07:14:17.861547       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1015 07:14:17.861605       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1015 07:14:17.861602       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1015 07:14:17.861664       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1015 07:14:17.861737       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:17.861736       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:17.861766       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:17.861767       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:17.861776       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:17.861784       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:17.861908       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1015 07:14:17.861938       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1015 07:14:17.861939       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1015 07:14:17.861973       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1015 07:14:17.954135       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1015 07:14:17.959638       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1015 07:14:17.959893       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1015 07:14:17.959940       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1015 07:14:18.760189       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1015 07:14:18.760250       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1015 07:14:18.860146       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1015 07:14:18.860183       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1015 07:14:18.972842       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:18.972879       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:19.060147       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:19.060226       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:19.060147       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1015 07:14:19.060276       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1015 07:14:19.083615       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1015 07:14:19.083653       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1015 07:14:19.163331       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1015 07:14:19.163381       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1015 07:14:19.260088       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1015 07:14:19.260115       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1015 07:14:19.361216       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1015 07:14:19.361284       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1015 07:14:19.370524       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1015 07:14:19.370566       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1015 07:14:19.380126       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1015 07:14:19.380194       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1015 07:14:19.460295       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1015 07:14:19.460349       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1015 07:14:19.460295       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:19.460400       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:19.663281       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:19.663343       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:19.760135       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1015 07:14:19.760172       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1015 07:14:20.810475       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:20.810511       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1015 07:14:20.860175       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1015 07:14:20.860213       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1015 07:14:21.254100       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1015 07:14:21.254139       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1015 07:14:21.400268       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1015 07:14:21.400333       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1015 07:14:21.460103       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1015 07:14:21.460144       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
I1015 07:14:25.658938       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Oct 15 10:00:04 minikube kubelet[2750]: W1015 10:00:04.216472    2750 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Oct 15 10:00:19 minikube kubelet[2750]: I1015 10:00:19.507969    2750 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-rr6dl" podStartSLOduration=3.719770326 podCreationTimestamp="2023-10-15 09:58:49 +0000 UTC" firstStartedPulling="2023-10-15 09:58:50.432569505 +0000 UTC m=+8328.986348416" lastFinishedPulling="2023-10-15 10:00:17.218440443 +0000 UTC m=+8415.774448326" observedRunningTime="2023-10-15 10:00:19.507637726 +0000 UTC m=+8418.060392864" watchObservedRunningTime="2023-10-15 10:00:19.507870236 +0000 UTC m=+8418.060625374"
Oct 15 10:05:04 minikube kubelet[2750]: W1015 10:05:04.223597    2750 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.117481    2750 topology_manager.go:212] "Topology Admit Handler"
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.232997    2750 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f5cbc\" (UniqueName: \"kubernetes.io/projected/3e019638-554f-454f-84fd-10e276090431-kube-api-access-f5cbc\") pod \"hello-minikube1-5c5f84495c-s5jrk\" (UID: \"3e019638-554f-454f-84fd-10e276090431\") " pod="default/hello-minikube1-5c5f84495c-s5jrk"
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.778426    2750 scope.go:115] "RemoveContainer" containerID="d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d"
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.837618    2750 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-llpjj\" (UniqueName: \"kubernetes.io/projected/9e413917-c637-466b-aa47-d984115c1ebd-kube-api-access-llpjj\") pod \"9e413917-c637-466b-aa47-d984115c1ebd\" (UID: \"9e413917-c637-466b-aa47-d984115c1ebd\") "
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.839310    2750 scope.go:115] "RemoveContainer" containerID="d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d"
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.841665    2750 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/9e413917-c637-466b-aa47-d984115c1ebd-kube-api-access-llpjj" (OuterVolumeSpecName: "kube-api-access-llpjj") pod "9e413917-c637-466b-aa47-d984115c1ebd" (UID: "9e413917-c637-466b-aa47-d984115c1ebd"). InnerVolumeSpecName "kube-api-access-llpjj". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 15 10:06:02 minikube kubelet[2750]: E1015 10:06:02.841792    2750 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d" containerID="d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d"
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.841870    2750 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d} err="failed to get container status \"d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d\": rpc error: code = Unknown desc = Error response from daemon: No such container: d6eb5e920bc120fdfc88ec67062d50d349ae4a65693b1221faff2a68b86ebe8d"
Oct 15 10:06:02 minikube kubelet[2750]: I1015 10:06:02.939712    2750 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-llpjj\" (UniqueName: \"kubernetes.io/projected/9e413917-c637-466b-aa47-d984115c1ebd-kube-api-access-llpjj\") on node \"minikube\" DevicePath \"\""
Oct 15 10:06:03 minikube kubelet[2750]: I1015 10:06:03.045883    2750 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1605fc22aae704b8268f946bbfee439bc995af9fa6f1a9f8496a5c4208f8cdec"
Oct 15 10:06:04 minikube kubelet[2750]: I1015 10:06:04.737880    2750 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=9e413917-c637-466b-aa47-d984115c1ebd path="/var/lib/kubelet/pods/9e413917-c637-466b-aa47-d984115c1ebd/volumes"
Oct 15 10:06:24 minikube kubelet[2750]: I1015 10:06:24.105861    2750 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/hello-minikube1-5c5f84495c-s5jrk" podStartSLOduration=22.105782416 podCreationTimestamp="2023-10-15 10:06:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-10-15 10:06:04.087512802 +0000 UTC m=+8762.633942698" watchObservedRunningTime="2023-10-15 10:06:24.105782416 +0000 UTC m=+8782.650876192"
Oct 15 10:06:24 minikube kubelet[2750]: I1015 10:06:24.106446    2750 topology_manager.go:212] "Topology Admit Handler"
Oct 15 10:06:24 minikube kubelet[2750]: E1015 10:06:24.106559    2750 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="9e413917-c637-466b-aa47-d984115c1ebd" containerName="echo-server"
Oct 15 10:06:24 minikube kubelet[2750]: I1015 10:06:24.106617    2750 memory_manager.go:346] "RemoveStaleState removing state" podUID="9e413917-c637-466b-aa47-d984115c1ebd" containerName="echo-server"
Oct 15 10:06:24 minikube kubelet[2750]: I1015 10:06:24.220807    2750 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jtkr2\" (UniqueName: \"kubernetes.io/projected/c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1-kube-api-access-jtkr2\") pod \"app-b-9f6f7b4cf-ngqr7\" (UID: \"c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1\") " pod="default/app-b-9f6f7b4cf-ngqr7"
Oct 15 10:06:28 minikube kubelet[2750]: E1015 10:06:28.609683    2750 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:06:28 minikube kubelet[2750]: E1015 10:06:28.609803    2750 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:06:28 minikube kubelet[2750]: E1015 10:06:28.609918    2750 kuberuntime_manager.go:1212] container &Container{Name:app-b,Image:app-b,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtkr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod app-b-9f6f7b4cf-ngqr7_default(c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Oct 15 10:06:28 minikube kubelet[2750]: E1015 10:06:28.609964    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:06:29 minikube kubelet[2750]: E1015 10:06:29.317100    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:06:46 minikube kubelet[2750]: E1015 10:06:46.496406    2750 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:06:46 minikube kubelet[2750]: E1015 10:06:46.496477    2750 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:06:46 minikube kubelet[2750]: E1015 10:06:46.496582    2750 kuberuntime_manager.go:1212] container &Container{Name:app-b,Image:app-b,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtkr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod app-b-9f6f7b4cf-ngqr7_default(c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Oct 15 10:06:46 minikube kubelet[2750]: E1015 10:06:46.496623    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:06:59 minikube kubelet[2750]: E1015 10:06:59.730474    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:07:17 minikube kubelet[2750]: E1015 10:07:17.811195    2750 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:07:17 minikube kubelet[2750]: E1015 10:07:17.811273    2750 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:07:17 minikube kubelet[2750]: E1015 10:07:17.811402    2750 kuberuntime_manager.go:1212] container &Container{Name:app-b,Image:app-b,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtkr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod app-b-9f6f7b4cf-ngqr7_default(c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Oct 15 10:07:17 minikube kubelet[2750]: E1015 10:07:17.811452    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:07:30 minikube kubelet[2750]: E1015 10:07:30.729153    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:07:41 minikube kubelet[2750]: E1015 10:07:41.729042    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:07:52 minikube kubelet[2750]: E1015 10:07:52.730870    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:08:10 minikube kubelet[2750]: E1015 10:08:10.739186    2750 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:08:10 minikube kubelet[2750]: E1015 10:08:10.739246    2750 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-b:latest"
Oct 15 10:08:10 minikube kubelet[2750]: E1015 10:08:10.739370    2750 kuberuntime_manager.go:1212] container &Container{Name:app-b,Image:app-b,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtkr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod app-b-9f6f7b4cf-ngqr7_default(c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Oct 15 10:08:10 minikube kubelet[2750]: E1015 10:08:10.739435    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:08:25 minikube kubelet[2750]: E1015 10:08:25.725134    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:08:37 minikube kubelet[2750]: E1015 10:08:37.725303    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:08:49 minikube kubelet[2750]: E1015 10:08:49.722829    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:09:00 minikube kubelet[2750]: E1015 10:09:00.724881    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:09:12 minikube kubelet[2750]: E1015 10:09:12.726168    2750 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-b\" with ImagePullBackOff: \"Back-off pulling image \\\"app-b\\\"\"" pod="default/app-b-9f6f7b4cf-ngqr7" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1
Oct 15 10:09:24 minikube kubelet[2750]: I1015 10:09:24.232668    2750 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jtkr2\" (UniqueName: \"kubernetes.io/projected/c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1-kube-api-access-jtkr2\") pod \"c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1\" (UID: \"c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1\") "
Oct 15 10:09:24 minikube kubelet[2750]: I1015 10:09:24.236977    2750 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1-kube-api-access-jtkr2" (OuterVolumeSpecName: "kube-api-access-jtkr2") pod "c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1" (UID: "c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1"). InnerVolumeSpecName "kube-api-access-jtkr2". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 15 10:09:24 minikube kubelet[2750]: I1015 10:09:24.333044    2750 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-jtkr2\" (UniqueName: \"kubernetes.io/projected/c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1-kube-api-access-jtkr2\") on node \"minikube\" DevicePath \"\""
Oct 15 10:09:26 minikube kubelet[2750]: I1015 10:09:26.730634    2750 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1 path="/var/lib/kubelet/pods/c4d467da-f6ef-4bb0-881a-f9eb6ed6d3c1/volumes"
Oct 15 10:09:40 minikube kubelet[2750]: I1015 10:09:40.343753    2750 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-f5cbc\" (UniqueName: \"kubernetes.io/projected/3e019638-554f-454f-84fd-10e276090431-kube-api-access-f5cbc\") pod \"3e019638-554f-454f-84fd-10e276090431\" (UID: \"3e019638-554f-454f-84fd-10e276090431\") "
Oct 15 10:09:40 minikube kubelet[2750]: I1015 10:09:40.347456    2750 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3e019638-554f-454f-84fd-10e276090431-kube-api-access-f5cbc" (OuterVolumeSpecName: "kube-api-access-f5cbc") pod "3e019638-554f-454f-84fd-10e276090431" (UID: "3e019638-554f-454f-84fd-10e276090431"). InnerVolumeSpecName "kube-api-access-f5cbc". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 15 10:09:40 minikube kubelet[2750]: I1015 10:09:40.446345    2750 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-f5cbc\" (UniqueName: \"kubernetes.io/projected/3e019638-554f-454f-84fd-10e276090431-kube-api-access-f5cbc\") on node \"minikube\" DevicePath \"\""
Oct 15 10:09:41 minikube kubelet[2750]: I1015 10:09:41.257043    2750 scope.go:115] "RemoveContainer" containerID="b2bc16b6a7d05f8a8edeb2bd356b4b7f0436b220eef1b49bf37d002e1fee5733"
Oct 15 10:09:42 minikube kubelet[2750]: I1015 10:09:42.730953    2750 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=3e019638-554f-454f-84fd-10e276090431 path="/var/lib/kubelet/pods/3e019638-554f-454f-84fd-10e276090431/volumes"
Oct 15 10:09:48 minikube kubelet[2750]: I1015 10:09:48.729724    2750 topology_manager.go:212] "Topology Admit Handler"
Oct 15 10:09:48 minikube kubelet[2750]: E1015 10:09:48.729797    2750 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3e019638-554f-454f-84fd-10e276090431" containerName="echo-server"
Oct 15 10:09:48 minikube kubelet[2750]: I1015 10:09:48.729833    2750 memory_manager.go:346] "RemoveStaleState removing state" podUID="3e019638-554f-454f-84fd-10e276090431" containerName="echo-server"
Oct 15 10:09:48 minikube kubelet[2750]: I1015 10:09:48.902219    2750 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-czx79\" (UniqueName: \"kubernetes.io/projected/b0cf39a5-405d-4556-8226-d10136932aff-kube-api-access-czx79\") pod \"web-548f6458b5-jlcr5\" (UID: \"b0cf39a5-405d-4556-8226-d10136932aff\") " pod="default/web-548f6458b5-jlcr5"
Oct 15 10:09:49 minikube kubelet[2750]: I1015 10:09:49.514859    2750 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="21dab93651e176ae42fb772ec3fc2b918b28d1859554759ad860e0a2f87c844d"
Oct 15 10:10:04 minikube kubelet[2750]: W1015 10:10:04.221931    2750 sysinfo.go:203] Nodes topology is not available, providing CPU topology


==> kubernetes-dashboard [305f9f3f0be8] <==
2023/10/15 10:03:38 [2023-10-15T10:03:38Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/10/15 10:03:38 Getting list of namespaces
2023/10/15 10:03:38 [2023-10-15T10:03:38Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/hello-minikube1 request from 127.0.0.1: 
2023/10/15 10:03:42 Getting details of hello-minikube1 deployment in default namespace
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/hello-minikube1/newreplicaset request from 127.0.0.1: 
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/hello-minikube1/oldreplicaset?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:42 Internal error occurred: No metric client provided. Skipping metrics.
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/hello-minikube1/horizontalpodautoscaler?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:42 Internal error occurred: No metric client provided. Skipping metrics.
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/hello-minikube1/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/10/15 10:03:42 [2023-10-15T10:03:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:43 [2023-10-15T10:03:43Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/10/15 10:03:43 Getting list of namespaces
2023/10/15 10:03:43 [2023-10-15T10:03:43Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:43 [2023-10-15T10:03:43Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:43 Getting list of all pods in the cluster
2023/10/15 10:03:43 received 0 resources from sidecar instead of 1
2023/10/15 10:03:43 received 0 resources from sidecar instead of 1
2023/10/15 10:03:43 Getting pod metrics
2023/10/15 10:03:43 received 0 resources from sidecar instead of 1
2023/10/15 10:03:43 received 0 resources from sidecar instead of 1
2023/10/15 10:03:43 Skipping metric because of error: Metric label not set.
2023/10/15 10:03:43 Skipping metric because of error: Metric label not set.
2023/10/15 10:03:43 [2023-10-15T10:03:43Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:45 [2023-10-15T10:03:45Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/10/15 10:03:45 [2023-10-15T10:03:45Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:45 [2023-10-15T10:03:45Z] Incoming HTTP/1.1 GET /api/v1/ingress/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:45 [2023-10-15T10:03:45Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:46 [2023-10-15T10:03:46Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/10/15 10:03:46 [2023-10-15T10:03:46Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:46 [2023-10-15T10:03:46Z] Incoming HTTP/1.1 GET /api/v1/ingressclass?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:46 Getting list of ingress classes in the cluster
2023/10/15 10:03:46 [2023-10-15T10:03:46Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:47 [2023-10-15T10:03:47Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/10/15 10:03:47 [2023-10-15T10:03:47Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:47 [2023-10-15T10:03:47Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:47 Getting list of all services in the cluster
2023/10/15 10:03:47 [2023-10-15T10:03:47Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:48 [2023-10-15T10:03:48Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/10/15 10:03:48 Getting list of namespaces
2023/10/15 10:03:48 [2023-10-15T10:03:48Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:52 [2023-10-15T10:03:52Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:52 Getting list of all services in the cluster
2023/10/15 10:03:52 [2023-10-15T10:03:52Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:53 [2023-10-15T10:03:53Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/10/15 10:03:53 Getting list of namespaces
2023/10/15 10:03:53 [2023-10-15T10:03:53Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:56 [2023-10-15T10:03:56Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/10/15 10:03:56 Getting list of namespaces
2023/10/15 10:03:56 [2023-10-15T10:03:56Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/10/15 10:03:56 Getting list of all services in the cluster
2023/10/15 10:03:56 [2023-10-15T10:03:56Z] Outcoming response to 127.0.0.1 with 200 status code
2023/10/15 10:03:56 [2023-10-15T10:03:56Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [97886c62076c] <==
I1015 07:14:46.861151       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1015 07:14:56.874317       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout


==> storage-provisioner [f1702b202a48] <==
I1015 07:14:58.271229       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1015 07:14:58.373180       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1015 07:14:58.373228       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1015 07:14:58.482202       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1015 07:14:58.482289       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"612b348e-df06-4eb7-aaf2-5608a900037f", APIVersion:"v1", ResourceVersion:"423", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_0e672b3a-2fd6-42d6-ae28-dbddd65be463 became leader
I1015 07:14:58.482344       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_0e672b3a-2fd6-42d6-ae28-dbddd65be463!
I1015 07:14:58.655376       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_0e672b3a-2fd6-42d6-ae28-dbddd65be463!

